{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hushem = './backend/datasets/HuSHem/'\n",
    "hushem_data_folder = datasets.ImageFolder(hushem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "train_size = int(0.8 * len(hushem_data_folder))\n",
    "test_size = len(hushem_data_folder) - train_size\n",
    "train_data, test_data = random_split(hushem_data_folder, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Charger les images avec ImageFolder\n",
    "# hushem_data = datasets.ImageFolder(root=hushem, transform=transform)\n",
    "\n",
    "# # Créer le dataloader\n",
    "# dataloader = DataLoader(hushem_data, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Vérifier si les données sont correctement chargées\n",
    "# for inputs, labels in dataloader:\n",
    "#     print(inputs.shape, labels.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les images avec ImageFolder\n",
    "hushem_data = datasets.ImageFolder(root=hushem, transform=transform)\n",
    "\n",
    "# Séparer les données en ensembles d'entraînement et de test\n",
    "train_size = int(0.8 * len(hushem_data))\n",
    "test_size = len(hushem_data) - train_size\n",
    "train_data, test_data = random_split(hushem_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les dataloaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\teren\\Documents\\GitHub\\Projet-AFH\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\teren\\Documents\\GitHub\\Projet-AFH\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, len(hushem_data.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choisir l'optimiseur\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraîner le modèle\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for inputs, labels in test_dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les métriques d'évaluation\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6506\n",
      "Recall: 0.6319\n",
      "F1 Score: 0.6183\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premier test avec entraînement sur 10 époques\n",
    "- Precision: 0.5917\n",
    "- Recall: 0.5824\n",
    "- F1 Score: 0.5674"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième test avec entraînement sur 30 époques\n",
    "- Precision: 0.7217\n",
    "- Recall: 0.7111\n",
    "- F1 Score: 0.6839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle semble avoir obtenu de meilleures performances après avoir augmenté le nombre d'époques.Voici ce que signifient les métriques de précision, rappel et score F1 :\n",
    "\n",
    "- **Précision (Precision) :** La précision mesure la proportion de vrais positifs parmi les prédictions positives totales. En d'autres termes, c'est la capacité du modèle à ne prédire comme positifs que les exemples qui le sont réellement.\n",
    "    Dans notre cas, une **précision de 0.7217** signifie que près de **72.17 % des prédictions positives** faites par notre modèle sont effectivement correctes.\n",
    "\n",
    "- **Rappel (Recall) :** Le rappel mesure la proportion de vrais positifs parmi les exemples réels positifs.\n",
    "    C'est la capacité du modèle à identifier tous les exemples positifs.\n",
    "    Avec un **rappel de 0.7111**, votre modèle a réussi à identifier environ **71.11 %** de tous les exemples positifs dans le jeu de données.\n",
    "\n",
    "- **Score F1 :** Le score F1 est une mesure combinée de la précision et du rappel, calculée comme la moyenne harmonique entre ces deux métriques.\n",
    "    Il est utile lorsque vous souhaitez trouver un équilibre entre la précision et le rappel.\n",
    "    Un **score F1 de 0.6839** indique que votre modèle atteint un **bon équilibre** entre la précision et le rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enregistrement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enregistrez le modèle\n",
    "# Supposons que vous ayez un modèle appelé 'model' déjà entraîné et prêt à être exporté\n",
    "dummy_input = torch.randn(32, 3, 224, 224)  # Remplacez ces valeurs par les dimensions de vos données\n",
    "onnx_filename = \"hushem_model.onnx\"\n",
    "# Exportez le modèle au format ONNX\n",
    "torch.onnx.export(model, dummy_input, onnx_filename, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = rt.InferenceSession('C:\\\\Users\\\\teren\\\\Documents\\\\GitHub\\\\Projet-AFH\\\\backend\\\\dashboard\\\\hushem_model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom de l'entrée: input.1\n",
      "Type de l'entrée: tensor(float)\n",
      "Forme de l'entrée: [32, 3, 224, 224]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtenir des informations sur les entrées du modèle\n",
    "inputs = sess.get_inputs()\n",
    "for input in inputs:\n",
    "    print(f\"Nom de l'entrée: {input.name}\")\n",
    "    print(f\"Type de l'entrée: {input.type}\")\n",
    "    print(f\"Forme de l'entrée: {input.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
